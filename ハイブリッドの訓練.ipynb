{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WgZBANyk_cZl"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/takhahashi/conf.git\n",
        "%cd 'conf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0NIB79f0tPY",
        "outputId": "d103852e-5344-4482-d693-ef698d58e82a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'conf'...\n",
            "remote: Enumerating objects: 706, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 706 (delta 44), reused 52 (delta 21), pack-reused 629 (from 1)\u001b[K\n",
            "Receiving objects: 100% (706/706), 87.36 KiB | 4.37 MiB/s, done.\n",
            "Resolving deltas: 100% (448/448), done.\n",
            "/content/conf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hydra-core --upgrade\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install orjson\n",
        "!pip install wget\n",
        "!pip install pytreebank\n",
        "!pip install accelerate\n",
        "!pip install -U transformers\n",
        "!pip install optuna\n",
        "!pip install fugashi\n",
        "!pip install unidic_lite\n",
        "!pip install ipadic\n",
        "!pip install gpytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lm_NPnw202tj",
        "outputId": "8a345117-5277-465b-a7f9-4327af46a6e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=b1a97499b8adce6d933208fbbac822057049f193b743446dde73c63ae09d41dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "51c3fbaf6a0a4046b03d21659bf9c3f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (3.10.15)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=31b845d07b2d8c6489ea3a9aca2bd33968588c9e33d0f564660129f43019c720\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting pytreebank\n",
            "  Downloading pytreebank-0.2.7.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-py3-none-any.whl size=37069 sha256=5d0915c8e734520e5095c52d8ac021babd265152837708412a1d74f8b32a66fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/76/35/53861f6ddf9b2a448cb66ffc44231e794a8e636d0a027b9d39\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.49.0\n",
            "    Uninstalling transformers-4.49.0:\n",
            "      Successfully uninstalled transformers-4.49.0\n",
            "Successfully installed transformers-4.50.0\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (698 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.0/698.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.4.0\n",
            "Collecting unidic_lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic_lite\n",
            "  Building wheel for unidic_lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic_lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=76eedcbfb2addf334d8d0841a5855bf0b19e12a72d40bb1550d51d8acc640a41\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/fd/e9/ea4459b868e6d2902e8d80e82dbacb6203e05b3b3a58c64966\n",
            "Successfully built unidic_lite\n",
            "Installing collected packages: unidic_lite\n",
            "Successfully installed unidic_lite-1.0.8\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=0211098249c0056d5ce4fa22d6ead643a43303c8fc501ac30d858d0d52dd62d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/56/37/f543963822b85260c9f948df8fac8c20169c80dc71b24dc407\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.14.1)\n",
            "Collecting linear-operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from linear-operator>=0.6->gpytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.2)\n",
            "Downloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, linear-operator, gpytorch\n",
            "Successfully installed gpytorch-1.14 jaxtyping-0.3.0 linear-operator-0.6 wadler-lindig-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ハイブリッドモデルの訓練"
      ],
      "metadata": {
        "id": "v-XivBOHCuFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 理研のデータセット"
      ],
      "metadata": {
        "id": "QKRuJMJ15NfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y14_1-2_1_3\n",
        "!python train.py -m data=riken data.question_id_pref='Y14' data.question_id_suff='1-2_1_3' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFnCl0by0_ef",
        "outputId": "0e4bbe91-1ac0-4067-b849-96ee1684d568"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-22 08:12:34.521195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742631154.558476    6766 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742631154.570445    6766 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-22 08:12:34.606667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/conf/train.py:183: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "[2025-03-22 08:12:39,717][HYDRA] Launching 5 jobs locally\n",
            "[2025-03-22 08:12:39,717][HYDRA] \t#0 : data=riken data.question_id_pref=Y14 data.question_id_suff=1-2_1_3 data.score_id=score data.fold=0 model.model_type=hybrid model.id=0 model.model_name_or_path=tohoku-nlp/bert-base-japanese-v3 +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/Y14_1-2_1_3/fold$\\{data.fold\\}/id$\\{model.id\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:12:39,820][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-12-39/0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "[2025-03-22 08:12:46,911][__main__][INFO] - config:{'data': {'task_name': 'riken', 'question_id_pref': 'Y14', 'question_id_suff': '1-2_1_3', 'score_id': 'score', 'fold': 0, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/SA/ShortAnswer/${data.question_id_pref}/${data.question_id_suff}_data/${data.question_id_pref}_${data.question_id_suff}_fold${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/Y14_1-2_1_3/fold${data.fold}/id${model.id}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'tohoku-nlp/bert-base-japanese-v3'}}\n",
            "[2025-03-22 08:12:46,911][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:12:47,056][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:12:48,078][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Parameter 'function'=<function train_model.<locals>.tokenize_function at 0x7c43dc558f40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "[2025-03-22 08:12:48,280][datasets.fingerprint][WARNING] - Parameter 'function'=<function train_model.<locals>.tokenize_function at 0x7c43dc558f40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Map: 100% 1260/1260 [00:00<00:00, 2015.30 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2090.30 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2062.22 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'regression_scaled_loss': np.float64(0.012669472955167294), 'regression_loss': np.float64(0.025338945910334587), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(1.1738685369491577), 'classification_loss': np.float64(2.3477370738983154), 'epoch': 1.0}\n",
            "100% 158/158 [00:22<00:00,  7.32it/s]\n",
            "  0% 0/53 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 4/53 [00:00<00:01, 37.17it/s]\u001b[A\n",
            " 15% 8/53 [00:00<00:01, 28.47it/s]\u001b[A\n",
            " 23% 12/53 [00:00<00:01, 29.49it/s]\u001b[A\n",
            " 30% 16/53 [00:00<00:01, 29.55it/s]\u001b[A\n",
            " 36% 19/53 [00:00<00:01, 29.23it/s]\u001b[A\n",
            " 42% 22/53 [00:00<00:01, 28.67it/s]\u001b[A\n",
            " 47% 25/53 [00:00<00:01, 27.55it/s]\u001b[A\n",
            " 53% 28/53 [00:00<00:00, 27.41it/s]\u001b[A\n",
            " 58% 31/53 [00:01<00:00, 27.51it/s]\u001b[A\n",
            " 64% 34/53 [00:01<00:00, 27.52it/s]\u001b[A\n",
            " 70% 37/53 [00:01<00:00, 28.04it/s]\u001b[A\n",
            " 75% 40/53 [00:01<00:00, 27.78it/s]\u001b[A\n",
            " 81% 43/53 [00:01<00:00, 28.03it/s]\u001b[A\n",
            " 87% 46/53 [00:01<00:00, 27.99it/s]\u001b[A\n",
            " 92% 49/53 [00:01<00:00, 27.64it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8802447114393603, 'eval_runtime': 1.9469, 'eval_samples_per_second': 215.73, 'eval_steps_per_second': 27.223, 'epoch': 1.0}\n",
            "100% 158/158 [00:24<00:00,  7.32it/s]\n",
            "100% 53/53 [00:01<00:00, 27.16it/s]\u001b[A\n",
            "{'train_runtime': 29.7197, 'train_samples_per_second': 42.396, 'train_steps_per_second': 5.316, 'train_loss': 1.188186162634741, 'epoch': 1.0}\n",
            "100% 158/158 [00:29<00:00,  5.32it/s]\n",
            "[2025-03-22 08:13:25,748][HYDRA] \t#1 : data=riken data.question_id_pref=Y14 data.question_id_suff=1-2_1_3 data.score_id=score data.fold=1 model.model_type=hybrid model.id=0 model.model_name_or_path=tohoku-nlp/bert-base-japanese-v3 +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/Y14_1-2_1_3/fold$\\{data.fold\\}/id$\\{model.id\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:13:25,896][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-12-39/1\n",
            "[2025-03-22 08:13:25,925][__main__][INFO] - config:{'data': {'task_name': 'riken', 'question_id_pref': 'Y14', 'question_id_suff': '1-2_1_3', 'score_id': 'score', 'fold': 1, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/SA/ShortAnswer/${data.question_id_pref}/${data.question_id_suff}_data/${data.question_id_pref}_${data.question_id_suff}_fold${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/Y14_1-2_1_3/fold${data.fold}/id${model.id}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'tohoku-nlp/bert-base-japanese-v3'}}\n",
            "[2025-03-22 08:13:25,925][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:13:26,080][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:13:26,997][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1260/1260 [00:01<00:00, 1098.42 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2040.91 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2002.85 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.011961791664361954), 'regression_loss': np.float64(0.023923583328723907), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(1.1562573909759521), 'classification_loss': np.float64(2.3125147819519043), 'epoch': 1.0}\n",
            "100% 158/158 [00:21<00:00,  7.99it/s]\n",
            "  0% 0/53 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 5/53 [00:00<00:01, 43.38it/s]\u001b[A\n",
            " 19% 10/53 [00:00<00:01, 37.88it/s]\u001b[A\n",
            " 26% 14/53 [00:00<00:01, 36.45it/s]\u001b[A\n",
            " 34% 18/53 [00:00<00:01, 33.65it/s]\u001b[A\n",
            " 42% 22/53 [00:00<00:00, 33.53it/s]\u001b[A\n",
            " 49% 26/53 [00:00<00:00, 34.23it/s]\u001b[A\n",
            " 57% 30/53 [00:00<00:00, 31.03it/s]\u001b[A\n",
            " 64% 34/53 [00:01<00:00, 29.36it/s]\u001b[A\n",
            " 70% 37/53 [00:01<00:00, 29.01it/s]\u001b[A\n",
            " 75% 40/53 [00:01<00:00, 28.37it/s]\u001b[A\n",
            " 81% 43/53 [00:01<00:00, 28.32it/s]\u001b[A\n",
            " 87% 46/53 [00:01<00:00, 28.36it/s]\u001b[A\n",
            " 92% 49/53 [00:01<00:00, 27.26it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9093995061192442, 'eval_runtime': 1.7742, 'eval_samples_per_second': 236.72, 'eval_steps_per_second': 29.872, 'epoch': 1.0}\n",
            "100% 158/158 [00:23<00:00,  7.99it/s]\n",
            "100% 53/53 [00:01<00:00, 29.76it/s]\u001b[A\n",
            "{'train_runtime': 32.2382, 'train_samples_per_second': 39.084, 'train_steps_per_second': 4.901, 'train_loss': 1.1697801034661788, 'epoch': 1.0}\n",
            "100% 158/158 [00:32<00:00,  4.90it/s]\n",
            "[2025-03-22 08:14:04,080][HYDRA] \t#2 : data=riken data.question_id_pref=Y14 data.question_id_suff=1-2_1_3 data.score_id=score data.fold=2 model.model_type=hybrid model.id=0 model.model_name_or_path=tohoku-nlp/bert-base-japanese-v3 +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/Y14_1-2_1_3/fold$\\{data.fold\\}/id$\\{model.id\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:14:04,180][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-12-39/2\n",
            "[2025-03-22 08:14:04,204][__main__][INFO] - config:{'data': {'task_name': 'riken', 'question_id_pref': 'Y14', 'question_id_suff': '1-2_1_3', 'score_id': 'score', 'fold': 2, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/SA/ShortAnswer/${data.question_id_pref}/${data.question_id_suff}_data/${data.question_id_pref}_${data.question_id_suff}_fold${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/Y14_1-2_1_3/fold${data.fold}/id${model.id}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'tohoku-nlp/bert-base-japanese-v3'}}\n",
            "[2025-03-22 08:14:04,205][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:14:04,321][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:14:05,045][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1260/1260 [00:00<00:00, 1983.20 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 1137.55 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 1134.89 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.010970917530357838), 'regression_loss': np.float64(0.021941835060715675), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(1.152750849723816), 'classification_loss': np.float64(2.305501699447632), 'epoch': 1.0}\n",
            "100% 158/158 [00:22<00:00,  7.85it/s]\n",
            "  0% 0/53 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 5/53 [00:00<00:01, 45.49it/s]\u001b[A\n",
            " 19% 10/53 [00:00<00:01, 37.45it/s]\u001b[A\n",
            " 26% 14/53 [00:00<00:01, 36.97it/s]\u001b[A\n",
            " 34% 18/53 [00:00<00:00, 35.88it/s]\u001b[A\n",
            " 42% 22/53 [00:00<00:00, 34.68it/s]\u001b[A\n",
            " 49% 26/53 [00:00<00:00, 34.12it/s]\u001b[A\n",
            " 57% 30/53 [00:00<00:00, 34.31it/s]\u001b[A\n",
            " 64% 34/53 [00:00<00:00, 35.26it/s]\u001b[A\n",
            " 72% 38/53 [00:01<00:00, 35.06it/s]\u001b[A\n",
            " 79% 42/53 [00:01<00:00, 34.05it/s]\u001b[A\n",
            " 87% 46/53 [00:01<00:00, 34.48it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7904974649393093, 'eval_runtime': 1.5568, 'eval_samples_per_second': 269.786, 'eval_steps_per_second': 34.044, 'epoch': 1.0}\n",
            "100% 158/158 [00:23<00:00,  7.85it/s]\n",
            "100% 53/53 [00:01<00:00, 34.36it/s]\u001b[A\n",
            "{'train_runtime': 33.8956, 'train_samples_per_second': 37.173, 'train_steps_per_second': 4.661, 'train_loss': 1.1649549460109276, 'epoch': 1.0}\n",
            "100% 158/158 [00:33<00:00,  4.66it/s]\n",
            "[2025-03-22 08:14:47,117][HYDRA] \t#3 : data=riken data.question_id_pref=Y14 data.question_id_suff=1-2_1_3 data.score_id=score data.fold=3 model.model_type=hybrid model.id=0 model.model_name_or_path=tohoku-nlp/bert-base-japanese-v3 +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/Y14_1-2_1_3/fold$\\{data.fold\\}/id$\\{model.id\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:14:47,273][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-12-39/3\n",
            "[2025-03-22 08:14:47,308][__main__][INFO] - config:{'data': {'task_name': 'riken', 'question_id_pref': 'Y14', 'question_id_suff': '1-2_1_3', 'score_id': 'score', 'fold': 3, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/SA/ShortAnswer/${data.question_id_pref}/${data.question_id_suff}_data/${data.question_id_pref}_${data.question_id_suff}_fold${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/Y14_1-2_1_3/fold${data.fold}/id${model.id}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'tohoku-nlp/bert-base-japanese-v3'}}\n",
            "[2025-03-22 08:14:47,308][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:14:47,464][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:14:48,391][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1260/1260 [00:00<00:00, 1504.17 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2033.90 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2103.62 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.011929770931601524), 'regression_loss': np.float64(0.02385954186320305), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(1.174933671951294), 'classification_loss': np.float64(2.349867343902588), 'epoch': 1.0}\n",
            "100% 158/158 [00:21<00:00,  7.78it/s]\n",
            "  0% 0/53 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 4/53 [00:00<00:01, 34.96it/s]\u001b[A\n",
            " 15% 8/53 [00:00<00:01, 30.66it/s]\u001b[A\n",
            " 23% 12/53 [00:00<00:01, 28.67it/s]\u001b[A\n",
            " 28% 15/53 [00:00<00:01, 28.37it/s]\u001b[A\n",
            " 34% 18/53 [00:00<00:01, 27.00it/s]\u001b[A\n",
            " 40% 21/53 [00:00<00:01, 27.43it/s]\u001b[A\n",
            " 45% 24/53 [00:00<00:01, 27.09it/s]\u001b[A\n",
            " 51% 27/53 [00:00<00:00, 27.82it/s]\u001b[A\n",
            " 57% 30/53 [00:01<00:00, 28.01it/s]\u001b[A\n",
            " 62% 33/53 [00:01<00:00, 28.42it/s]\u001b[A\n",
            " 68% 36/53 [00:01<00:00, 28.42it/s]\u001b[A\n",
            " 74% 39/53 [00:01<00:00, 27.92it/s]\u001b[A\n",
            " 79% 42/53 [00:01<00:00, 27.13it/s]\u001b[A\n",
            " 85% 45/53 [00:01<00:00, 26.47it/s]\u001b[A\n",
            " 91% 48/53 [00:01<00:00, 27.02it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8238187049193982, 'eval_runtime': 1.9801, 'eval_samples_per_second': 212.111, 'eval_steps_per_second': 26.766, 'epoch': 1.0}\n",
            "100% 158/158 [00:23<00:00,  7.78it/s]\n",
            "100% 53/53 [00:01<00:00, 26.54it/s]\u001b[A\n",
            "{'train_runtime': 30.1751, 'train_samples_per_second': 41.756, 'train_steps_per_second': 5.236, 'train_loss': 1.188222184965882, 'epoch': 1.0}\n",
            "100% 158/158 [00:30<00:00,  5.24it/s]\n",
            "[2025-03-22 08:15:24,162][HYDRA] \t#4 : data=riken data.question_id_pref=Y14 data.question_id_suff=1-2_1_3 data.score_id=score data.fold=4 model.model_type=hybrid model.id=0 model.model_name_or_path=tohoku-nlp/bert-base-japanese-v3 +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/Y14_1-2_1_3/fold$\\{data.fold\\}/id$\\{model.id\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:15:24,279][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-12-39/4\n",
            "[2025-03-22 08:15:24,304][__main__][INFO] - config:{'data': {'task_name': 'riken', 'question_id_pref': 'Y14', 'question_id_suff': '1-2_1_3', 'score_id': 'score', 'fold': 4, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/SA/ShortAnswer/${data.question_id_pref}/${data.question_id_suff}_data/${data.question_id_pref}_${data.question_id_suff}_fold${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/Y14_1-2_1_3/fold${data.fold}/id${model.id}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'tohoku-nlp/bert-base-japanese-v3'}}\n",
            "[2025-03-22 08:15:24,304][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:15:24,453][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:15:25,210][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1260/1260 [00:00<00:00, 1975.72 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2099.07 examples/s]\n",
            "Map: 100% 420/420 [00:00<00:00, 2103.94 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.01163171324878931), 'regression_loss': np.float64(0.02326342649757862), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(1.156615972518921), 'classification_loss': np.float64(2.313231945037842), 'epoch': 1.0}\n",
            "100% 158/158 [00:22<00:00,  7.79it/s]\n",
            "  0% 0/53 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 5/53 [00:00<00:01, 43.16it/s]\u001b[A\n",
            " 19% 10/53 [00:00<00:01, 38.04it/s]\u001b[A\n",
            " 26% 14/53 [00:00<00:01, 35.60it/s]\u001b[A\n",
            " 34% 18/53 [00:00<00:01, 34.84it/s]\u001b[A\n",
            " 42% 22/53 [00:00<00:00, 34.55it/s]\u001b[A\n",
            " 49% 26/53 [00:00<00:00, 34.11it/s]\u001b[A\n",
            " 57% 30/53 [00:00<00:00, 34.52it/s]\u001b[A\n",
            " 64% 34/53 [00:00<00:00, 33.82it/s]\u001b[A\n",
            " 72% 38/53 [00:01<00:00, 33.90it/s]\u001b[A\n",
            " 79% 42/53 [00:01<00:00, 34.09it/s]\u001b[A\n",
            " 87% 46/53 [00:01<00:00, 33.87it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9016438556071639, 'eval_runtime': 1.5795, 'eval_samples_per_second': 265.904, 'eval_steps_per_second': 33.555, 'epoch': 1.0}\n",
            "100% 158/158 [00:23<00:00,  7.79it/s]\n",
            "100% 53/53 [00:01<00:00, 33.24it/s]\u001b[A\n",
            "{'train_runtime': 31.8926, 'train_samples_per_second': 39.508, 'train_steps_per_second': 4.954, 'train_loss': 1.1699613740172567, 'epoch': 1.0}\n",
            "100% 158/158 [00:31<00:00,  4.96it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/conf/wandb/offline-run-20250322_081246-upkuyrz3\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250322_081246-upkuyrz3/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y14_1-2_2_4\n",
        "!python train.py -m data=riken data.question_id_pref='Y14' data.question_id_suff='1-2_2_4' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.0028 +training.margin=2.343 +training.lamb_intra=0.028 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "yJJNAik15brv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y14_2-1_1_5\n",
        "!python train.py -m data=riken data.question_id_pref='Y14' data.question_id_suff='2-1_1_5' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.739 +training.margin=0.225 +training.lamb_intra=7.39 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "eYSIymkb7ZAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y14_2-1_2_3\n",
        "!python train.py -m data=riken data.question_id_pref='Y14' data.question_id_suff='2-1_2_3' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.0125 +training.margin=0.271 +training.lamb_intra=0.125 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "-d5wd2188fnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y14_2-2_1_4\n",
        "!python train.py -m data=riken data.question_id_pref='Y14' data.question_id_suff='2-2_1_4' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.1632 +training.margin=1.907 +training.lamb_intra=1.632 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "F4pHzAJX9Cus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y14_2-2_2_3\n",
        "!python train.py -m data=riken data.question_id_pref='Y14' data.question_id_suff='2-2_2_3' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.3677 +training.margin=0.906 +training.lamb_intra=3.677 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "aGGbaB-99iV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y15_2-3_1_5\n",
        "!python train.py -m data=riken data.question_id_pref='Y15' data.question_id_suff='2-3_1_5' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.9525 +training.margin=5.343 +training.lamb_intra=9.525 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "lfpyGOsA94ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y15_2-3_2_2\n",
        "!python train.py -m data=riken data.question_id_pref='Y15' data.question_id_suff='2-3_2_2' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.0308 +training.margin=3.797 +training.lamb_intra=0.308 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "4FJN-kZS-yU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Y15_2-3_2_4\n",
        "!python train.py -m data=riken data.question_id_pref='Y15' data.question_id_suff='2-3_2_4' data.score_id='score' data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='tohoku-nlp/bert-base-japanese-v3' +training.lamb=0.1176 +training.margin=9.429 +training.lamb_intra=1.176 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/${data.question_id_pref}_${data.question_id_suff}/fold${data.fold}/id${model.id}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "UhvQE6Ic_Mvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASAPデータセット"
      ],
      "metadata": {
        "id": "WgZBANyk_cZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_1\n",
        "!python train.py -m data=asap data.prompt_id=1 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.122 +training.margin=1.311 +training.lamb_intra=1.22 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkIP7SD8_f7q",
        "outputId": "06f59756-8ad8-4b2b-eb31-79575fb0013e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-22 08:38:08.370045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742632688.390384   13358 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742632688.396934   13358 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-22 08:38:08.418278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/conf/train.py:183: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "[2025-03-22 08:38:13,945][HYDRA] Launching 5 jobs locally\n",
            "[2025-03-22 08:38:13,946][HYDRA] \t#0 : data=asap data.prompt_id=1 data.fold=0 model.model_type=hybrid model.id=0 model.model_name_or_path=bert-base-uncased +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/prompt_$\\{data.prompt_id\\}/fold$\\{data.fold\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:38:14,090][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-38-13/0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "[2025-03-22 08:38:18,433][__main__][INFO] - config:{'data': {'task_name': 'asap', 'prompt_id': 1, 'fold': 0, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/1.AES/ASAP/data/fold_${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'bert-base-uncased'}}\n",
            "[2025-03-22 08:38:18,433][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:38:22,561][__main__][INFO] - Done with loading the dataset.\n",
            "config.json: 100% 570/570 [00:00<00:00, 1.80MB/s]\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 162kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 2.79MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.85MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:01<00:00, 230MB/s]\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:38:26,545][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1070/1070 [00:02<00:00, 435.85 examples/s]\n",
            "Map: 100% 356/356 [00:00<00:00, 553.40 examples/s]\n",
            "Map: 100% 357/357 [00:00<00:00, 806.82 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "{'regression_scaled_loss': np.float64(0.013395835645496845), 'regression_loss': np.float64(0.02679167129099369), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(0.9103759527206421), 'classification_loss': np.float64(1.8207519054412842), 'epoch': 1.0}\n",
            "100% 134/134 [01:37<00:00,  1.43it/s]\n",
            "  0% 0/45 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/45 [00:00<00:04,  9.26it/s]\u001b[A\n",
            "  7% 3/45 [00:00<00:06,  6.46it/s]\u001b[A\n",
            "  9% 4/45 [00:00<00:07,  5.62it/s]\u001b[A\n",
            " 11% 5/45 [00:00<00:07,  5.20it/s]\u001b[A\n",
            " 13% 6/45 [00:01<00:07,  4.94it/s]\u001b[A\n",
            " 16% 7/45 [00:01<00:07,  4.80it/s]\u001b[A\n",
            " 18% 8/45 [00:01<00:07,  4.69it/s]\u001b[A\n",
            " 20% 9/45 [00:01<00:07,  4.63it/s]\u001b[A\n",
            " 22% 10/45 [00:01<00:07,  4.60it/s]\u001b[A\n",
            " 24% 11/45 [00:02<00:07,  4.58it/s]\u001b[A\n",
            " 27% 12/45 [00:02<00:07,  4.54it/s]\u001b[A\n",
            " 29% 13/45 [00:02<00:07,  4.53it/s]\u001b[A\n",
            " 31% 14/45 [00:02<00:06,  4.52it/s]\u001b[A\n",
            " 33% 15/45 [00:03<00:06,  4.51it/s]\u001b[A\n",
            " 36% 16/45 [00:03<00:06,  4.52it/s]\u001b[A\n",
            " 38% 17/45 [00:03<00:06,  4.51it/s]\u001b[A\n",
            " 40% 18/45 [00:03<00:05,  4.51it/s]\u001b[A\n",
            " 42% 19/45 [00:03<00:05,  4.48it/s]\u001b[A\n",
            " 44% 20/45 [00:04<00:05,  4.50it/s]\u001b[A\n",
            " 47% 21/45 [00:04<00:05,  4.50it/s]\u001b[A\n",
            " 49% 22/45 [00:04<00:05,  4.51it/s]\u001b[A\n",
            " 51% 23/45 [00:04<00:04,  4.50it/s]\u001b[A\n",
            " 53% 24/45 [00:05<00:04,  4.51it/s]\u001b[A\n",
            " 56% 25/45 [00:05<00:04,  4.52it/s]\u001b[A\n",
            " 58% 26/45 [00:05<00:04,  4.51it/s]\u001b[A\n",
            " 60% 27/45 [00:05<00:03,  4.53it/s]\u001b[A\n",
            " 62% 28/45 [00:05<00:03,  4.50it/s]\u001b[A\n",
            " 64% 29/45 [00:06<00:03,  4.50it/s]\u001b[A\n",
            " 67% 30/45 [00:06<00:03,  4.47it/s]\u001b[A\n",
            " 69% 31/45 [00:06<00:03,  4.45it/s]\u001b[A\n",
            " 71% 32/45 [00:06<00:02,  4.42it/s]\u001b[A\n",
            " 73% 33/45 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 34/45 [00:07<00:02,  4.40it/s]\u001b[A\n",
            " 78% 35/45 [00:07<00:02,  4.40it/s]\u001b[A\n",
            " 80% 36/45 [00:07<00:02,  4.39it/s]\u001b[A\n",
            " 82% 37/45 [00:08<00:01,  4.41it/s]\u001b[A\n",
            " 84% 38/45 [00:08<00:01,  4.40it/s]\u001b[A\n",
            " 87% 39/45 [00:08<00:01,  4.38it/s]\u001b[A\n",
            " 89% 40/45 [00:08<00:01,  4.37it/s]\u001b[A\n",
            " 91% 41/45 [00:08<00:00,  4.36it/s]\u001b[A\n",
            " 93% 42/45 [00:09<00:00,  4.40it/s]\u001b[A\n",
            " 96% 43/45 [00:09<00:00,  4.23it/s]\u001b[A\n",
            " 98% 44/45 [00:09<00:00,  4.32it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.69027129080529, 'eval_runtime': 10.0461, 'eval_samples_per_second': 35.437, 'eval_steps_per_second': 4.479, 'epoch': 1.0}\n",
            "100% 134/134 [01:47<00:00,  1.43it/s]\n",
            "100% 45/45 [00:09<00:00,  5.07it/s]\u001b[A\n",
            "{'train_runtime': 112.8492, 'train_samples_per_second': 9.482, 'train_steps_per_second': 1.187, 'train_loss': 0.9340957527730003, 'epoch': 1.0}\n",
            "100% 134/134 [01:52<00:00,  1.19it/s]\n",
            "[2025-03-22 08:40:27,541][HYDRA] \t#1 : data=asap data.prompt_id=1 data.fold=1 model.model_type=hybrid model.id=0 model.model_name_or_path=bert-base-uncased +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/prompt_$\\{data.prompt_id\\}/fold$\\{data.fold\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:40:27,639][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-38-13/1\n",
            "[2025-03-22 08:40:27,666][__main__][INFO] - config:{'data': {'task_name': 'asap', 'prompt_id': 1, 'fold': 1, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/1.AES/ASAP/data/fold_${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'bert-base-uncased'}}\n",
            "[2025-03-22 08:40:27,666][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:40:30,551][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:40:30,937][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1068/1068 [00:01<00:00, 760.47 examples/s]\n",
            "Map: 100% 357/357 [00:00<00:00, 799.12 examples/s]\n",
            "Map: 100% 357/357 [00:00<00:00, 806.95 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.012859726324677467), 'regression_loss': np.float64(0.025719452649354935), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(0.9019936323165894), 'classification_loss': np.float64(1.8039872646331787), 'epoch': 1.0}\n",
            "100% 134/134 [01:41<00:00,  1.49it/s]\n",
            "  0% 0/45 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/45 [00:00<00:04,  8.78it/s]\u001b[A\n",
            "  7% 3/45 [00:00<00:06,  6.14it/s]\u001b[A\n",
            "  9% 4/45 [00:00<00:07,  5.21it/s]\u001b[A\n",
            " 11% 5/45 [00:00<00:08,  4.81it/s]\u001b[A\n",
            " 13% 6/45 [00:01<00:08,  4.59it/s]\u001b[A\n",
            " 16% 7/45 [00:01<00:08,  4.45it/s]\u001b[A\n",
            " 18% 8/45 [00:01<00:08,  4.36it/s]\u001b[A\n",
            " 20% 9/45 [00:01<00:08,  4.27it/s]\u001b[A\n",
            " 22% 10/45 [00:02<00:08,  4.24it/s]\u001b[A\n",
            " 24% 11/45 [00:02<00:08,  4.24it/s]\u001b[A\n",
            " 27% 12/45 [00:02<00:07,  4.19it/s]\u001b[A\n",
            " 29% 13/45 [00:02<00:07,  4.19it/s]\u001b[A\n",
            " 31% 14/45 [00:03<00:07,  4.17it/s]\u001b[A\n",
            " 33% 15/45 [00:03<00:07,  4.17it/s]\u001b[A\n",
            " 36% 16/45 [00:03<00:07,  4.10it/s]\u001b[A\n",
            " 38% 17/45 [00:03<00:06,  4.14it/s]\u001b[A\n",
            " 40% 18/45 [00:04<00:06,  4.15it/s]\u001b[A\n",
            " 42% 19/45 [00:04<00:06,  4.13it/s]\u001b[A\n",
            " 44% 20/45 [00:04<00:06,  4.13it/s]\u001b[A\n",
            " 47% 21/45 [00:04<00:05,  4.16it/s]\u001b[A\n",
            " 49% 22/45 [00:05<00:05,  4.19it/s]\u001b[A\n",
            " 51% 23/45 [00:05<00:05,  4.22it/s]\u001b[A\n",
            " 53% 24/45 [00:05<00:04,  4.26it/s]\u001b[A\n",
            " 56% 25/45 [00:05<00:04,  4.25it/s]\u001b[A\n",
            " 58% 26/45 [00:05<00:04,  4.24it/s]\u001b[A\n",
            " 60% 27/45 [00:06<00:04,  4.28it/s]\u001b[A\n",
            " 62% 28/45 [00:06<00:03,  4.29it/s]\u001b[A\n",
            " 64% 29/45 [00:06<00:03,  4.28it/s]\u001b[A\n",
            " 67% 30/45 [00:06<00:03,  4.28it/s]\u001b[A\n",
            " 69% 31/45 [00:07<00:03,  4.28it/s]\u001b[A\n",
            " 71% 32/45 [00:07<00:03,  4.30it/s]\u001b[A\n",
            " 73% 33/45 [00:07<00:02,  4.29it/s]\u001b[A\n",
            " 76% 34/45 [00:07<00:02,  4.28it/s]\u001b[A\n",
            " 78% 35/45 [00:08<00:02,  4.27it/s]\u001b[A\n",
            " 80% 36/45 [00:08<00:02,  4.27it/s]\u001b[A\n",
            " 82% 37/45 [00:08<00:01,  4.28it/s]\u001b[A\n",
            " 84% 38/45 [00:08<00:01,  4.27it/s]\u001b[A\n",
            " 87% 39/45 [00:09<00:01,  4.25it/s]\u001b[A\n",
            " 89% 40/45 [00:09<00:01,  4.27it/s]\u001b[A\n",
            " 91% 41/45 [00:09<00:00,  4.29it/s]\u001b[A\n",
            " 93% 42/45 [00:09<00:00,  4.29it/s]\u001b[A\n",
            " 96% 43/45 [00:09<00:00,  4.27it/s]\u001b[A\n",
            " 98% 44/45 [00:10<00:00,  4.30it/s]\u001b[A\n",
            "100% 45/45 [00:10<00:00,  4.90it/s]\u001b[A\n",
            "{'eval_loss': 0.6708968782282311, 'eval_runtime': 10.575, 'eval_samples_per_second': 33.759, 'eval_steps_per_second': 4.255, 'epoch': 1.0}\n",
            "\n",
            "100% 134/134 [01:51<00:00,  1.49it/s]\n",
            "{'train_runtime': 121.2767, 'train_samples_per_second': 8.806, 'train_steps_per_second': 1.105, 'train_loss': 0.9259414672851562, 'epoch': 1.0}\n",
            "100% 134/134 [02:01<00:00,  1.10it/s]\n",
            "[2025-03-22 08:42:41,941][HYDRA] \t#2 : data=asap data.prompt_id=1 data.fold=2 model.model_type=hybrid model.id=0 model.model_name_or_path=bert-base-uncased +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/prompt_$\\{data.prompt_id\\}/fold$\\{data.fold\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:42:42,036][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-38-13/2\n",
            "[2025-03-22 08:42:42,061][__main__][INFO] - config:{'data': {'task_name': 'asap', 'prompt_id': 1, 'fold': 2, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/1.AES/ASAP/data/fold_${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'bert-base-uncased'}}\n",
            "[2025-03-22 08:42:42,061][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:42:44,912][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:42:45,514][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1068/1068 [00:02<00:00, 524.58 examples/s]\n",
            "Map: 100% 357/357 [00:00<00:00, 780.75 examples/s]\n",
            "Map: 100% 357/357 [00:00<00:00, 812.32 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.014157523401081562), 'regression_loss': np.float64(0.028315046802163124), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(0.91573166847229), 'classification_loss': np.float64(1.83146333694458), 'epoch': 1.0}\n",
            "100% 134/134 [01:42<00:00,  1.51it/s]\n",
            "  0% 0/45 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/45 [00:00<00:04,  9.22it/s]\u001b[A\n",
            "  7% 3/45 [00:00<00:06,  6.02it/s]\u001b[A\n",
            "  9% 4/45 [00:00<00:08,  5.05it/s]\u001b[A\n",
            " 11% 5/45 [00:00<00:08,  4.63it/s]\u001b[A\n",
            " 13% 6/45 [00:01<00:08,  4.38it/s]\u001b[A\n",
            " 16% 7/45 [00:01<00:08,  4.27it/s]\u001b[A\n",
            " 18% 8/45 [00:01<00:08,  4.22it/s]\u001b[A\n",
            " 20% 9/45 [00:01<00:08,  4.15it/s]\u001b[A\n",
            " 22% 10/45 [00:02<00:08,  4.15it/s]\u001b[A\n",
            " 24% 11/45 [00:02<00:08,  4.11it/s]\u001b[A\n",
            " 27% 12/45 [00:02<00:08,  4.12it/s]\u001b[A\n",
            " 29% 13/45 [00:02<00:07,  4.10it/s]\u001b[A\n",
            " 31% 14/45 [00:03<00:07,  4.13it/s]\u001b[A\n",
            " 33% 15/45 [00:03<00:07,  4.10it/s]\u001b[A\n",
            " 36% 16/45 [00:03<00:07,  4.11it/s]\u001b[A\n",
            " 38% 17/45 [00:03<00:06,  4.10it/s]\u001b[A\n",
            " 40% 18/45 [00:04<00:06,  3.97it/s]\u001b[A\n",
            " 42% 19/45 [00:04<00:06,  4.04it/s]\u001b[A\n",
            " 44% 20/45 [00:04<00:06,  4.05it/s]\u001b[A\n",
            " 47% 21/45 [00:04<00:05,  4.04it/s]\u001b[A\n",
            " 49% 22/45 [00:05<00:05,  4.08it/s]\u001b[A\n",
            " 51% 23/45 [00:05<00:05,  4.14it/s]\u001b[A\n",
            " 53% 24/45 [00:05<00:05,  4.17it/s]\u001b[A\n",
            " 56% 25/45 [00:05<00:04,  4.12it/s]\u001b[A\n",
            " 58% 26/45 [00:06<00:04,  4.13it/s]\u001b[A\n",
            " 60% 27/45 [00:06<00:04,  4.15it/s]\u001b[A\n",
            " 62% 28/45 [00:06<00:04,  4.14it/s]\u001b[A\n",
            " 64% 29/45 [00:06<00:03,  4.13it/s]\u001b[A\n",
            " 67% 30/45 [00:07<00:03,  4.13it/s]\u001b[A\n",
            " 69% 31/45 [00:07<00:03,  4.17it/s]\u001b[A\n",
            " 71% 32/45 [00:07<00:03,  4.17it/s]\u001b[A\n",
            " 73% 33/45 [00:07<00:02,  4.17it/s]\u001b[A\n",
            " 76% 34/45 [00:08<00:02,  4.16it/s]\u001b[A\n",
            " 78% 35/45 [00:08<00:02,  4.16it/s]\u001b[A\n",
            " 80% 36/45 [00:08<00:02,  4.15it/s]\u001b[A\n",
            " 82% 37/45 [00:08<00:01,  4.13it/s]\u001b[A\n",
            " 84% 38/45 [00:09<00:01,  4.13it/s]\u001b[A\n",
            " 87% 39/45 [00:09<00:01,  4.15it/s]\u001b[A\n",
            " 89% 40/45 [00:09<00:01,  4.16it/s]\u001b[A\n",
            " 91% 41/45 [00:09<00:00,  4.17it/s]\u001b[A\n",
            " 93% 42/45 [00:09<00:00,  4.17it/s]\u001b[A\n",
            " 96% 43/45 [00:10<00:00,  4.16it/s]\u001b[A\n",
            " 98% 44/45 [00:10<00:00,  4.15it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.6716415267509792, 'eval_runtime': 10.8719, 'eval_samples_per_second': 32.837, 'eval_steps_per_second': 4.139, 'epoch': 1.0}\n",
            "100% 134/134 [01:53<00:00,  1.51it/s]\n",
            "100% 45/45 [00:10<00:00,  4.70it/s]\u001b[A\n",
            "{'train_runtime': 120.2846, 'train_samples_per_second': 8.879, 'train_steps_per_second': 1.114, 'train_loss': 0.9435349649457789, 'epoch': 1.0}\n",
            "100% 134/134 [02:00<00:00,  1.11it/s]\n",
            "[2025-03-22 08:44:51,428][HYDRA] \t#3 : data=asap data.prompt_id=1 data.fold=3 model.model_type=hybrid model.id=0 model.model_name_or_path=bert-base-uncased +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/prompt_$\\{data.prompt_id\\}/fold$\\{data.fold\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:44:51,549][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-38-13/3\n",
            "[2025-03-22 08:44:51,579][__main__][INFO] - config:{'data': {'task_name': 'asap', 'prompt_id': 1, 'fold': 3, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/1.AES/ASAP/data/fold_${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'bert-base-uncased'}}\n",
            "[2025-03-22 08:44:51,579][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:44:54,204][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:44:54,526][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1070/1070 [00:01<00:00, 761.05 examples/s]\n",
            "Map: 100% 357/357 [00:00<00:00, 763.74 examples/s]\n",
            "Map: 100% 356/356 [00:00<00:00, 808.14 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.0130448704585433), 'regression_loss': np.float64(0.0260897409170866), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(0.8969030976295471), 'classification_loss': np.float64(1.7938061952590942), 'epoch': 1.0}\n",
            "100% 134/134 [01:44<00:00,  1.35it/s]\n",
            "  0% 0/45 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/45 [00:00<00:04,  8.76it/s]\u001b[A\n",
            "  7% 3/45 [00:00<00:06,  6.08it/s]\u001b[A\n",
            "  9% 4/45 [00:00<00:08,  5.05it/s]\u001b[A\n",
            " 11% 5/45 [00:00<00:08,  4.60it/s]\u001b[A\n",
            " 13% 6/45 [00:01<00:08,  4.49it/s]\u001b[A\n",
            " 16% 7/45 [00:01<00:08,  4.41it/s]\u001b[A\n",
            " 18% 8/45 [00:01<00:08,  4.32it/s]\u001b[A\n",
            " 20% 9/45 [00:01<00:08,  4.22it/s]\u001b[A\n",
            " 22% 10/45 [00:02<00:08,  4.20it/s]\u001b[A\n",
            " 24% 11/45 [00:02<00:08,  4.18it/s]\u001b[A\n",
            " 27% 12/45 [00:02<00:07,  4.18it/s]\u001b[A\n",
            " 29% 13/45 [00:02<00:07,  4.14it/s]\u001b[A\n",
            " 31% 14/45 [00:03<00:07,  4.13it/s]\u001b[A\n",
            " 33% 15/45 [00:03<00:07,  4.13it/s]\u001b[A\n",
            " 36% 16/45 [00:03<00:07,  4.14it/s]\u001b[A\n",
            " 38% 17/45 [00:03<00:06,  4.12it/s]\u001b[A\n",
            " 40% 18/45 [00:04<00:06,  4.11it/s]\u001b[A\n",
            " 42% 19/45 [00:04<00:06,  4.12it/s]\u001b[A\n",
            " 44% 20/45 [00:04<00:06,  4.12it/s]\u001b[A\n",
            " 47% 21/45 [00:04<00:05,  4.11it/s]\u001b[A\n",
            " 49% 22/45 [00:05<00:05,  4.11it/s]\u001b[A\n",
            " 51% 23/45 [00:05<00:05,  4.11it/s]\u001b[A\n",
            " 53% 24/45 [00:05<00:05,  4.10it/s]\u001b[A\n",
            " 56% 25/45 [00:05<00:04,  4.09it/s]\u001b[A\n",
            " 58% 26/45 [00:06<00:04,  4.08it/s]\u001b[A\n",
            " 60% 27/45 [00:06<00:04,  4.09it/s]\u001b[A\n",
            " 62% 28/45 [00:06<00:04,  4.08it/s]\u001b[A\n",
            " 64% 29/45 [00:06<00:04,  3.99it/s]\u001b[A\n",
            " 67% 30/45 [00:07<00:03,  3.98it/s]\u001b[A\n",
            " 69% 31/45 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 71% 32/45 [00:07<00:03,  4.04it/s]\u001b[A\n",
            " 73% 33/45 [00:07<00:02,  4.04it/s]\u001b[A\n",
            " 76% 34/45 [00:08<00:02,  4.03it/s]\u001b[A\n",
            " 78% 35/45 [00:08<00:02,  4.05it/s]\u001b[A\n",
            " 80% 36/45 [00:08<00:02,  4.06it/s]\u001b[A\n",
            " 82% 37/45 [00:08<00:01,  4.04it/s]\u001b[A\n",
            " 84% 38/45 [00:09<00:01,  4.02it/s]\u001b[A\n",
            " 87% 39/45 [00:09<00:01,  4.01it/s]\u001b[A\n",
            " 89% 40/45 [00:09<00:01,  4.01it/s]\u001b[A\n",
            " 91% 41/45 [00:09<00:00,  4.06it/s]\u001b[A\n",
            " 93% 42/45 [00:10<00:00,  4.06it/s]\u001b[A\n",
            " 96% 43/45 [00:10<00:00,  4.08it/s]\u001b[A\n",
            " 98% 44/45 [00:10<00:00,  4.10it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7069923901629644, 'eval_runtime': 10.9766, 'eval_samples_per_second': 32.524, 'eval_steps_per_second': 4.1, 'epoch': 1.0}\n",
            "100% 134/134 [01:55<00:00,  1.35it/s]\n",
            "100% 45/45 [00:10<00:00,  4.70it/s]\u001b[A\n",
            "{'train_runtime': 123.6796, 'train_samples_per_second': 8.651, 'train_steps_per_second': 1.083, 'train_loss': 0.926781554720295, 'epoch': 1.0}\n",
            "100% 134/134 [02:03<00:00,  1.08it/s]\n",
            "[2025-03-22 08:47:02,737][HYDRA] \t#4 : data=asap data.prompt_id=1 data.fold=4 model.model_type=hybrid model.id=0 model.model_name_or_path=bert-base-uncased +training.lamb=0.333 +training.margin=3.093 +training.lamb_intra=3.339 training.num_train_epochs=1 training.output_dir=/content/hybrid_bert/prompt_$\\{data.prompt_id\\}/fold$\\{data.fold\\} training.wandb_project=test training.wandb_runname=test\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[2025-03-22 08:47:02,890][__main__][INFO] - Work dir: /content/conf/multirun/2025-03-22/08-38-13/4\n",
            "[2025-03-22 08:47:02,927][__main__][INFO] - config:{'data': {'task_name': 'asap', 'prompt_id': 1, 'fold': 4, 'max_seq_length': 512, 'data_path': '/content/drive/MyDrive/GoogleColab/1.AES/ASAP/data/fold_${data.fold}'}, 'training': {'weight_decay': 0.0, 'learning_rate': 1e-05, 'per_device_eval_batch_size': 8, 'per_device_train_batch_size': 8, 'num_train_epochs': 1, 'lr_scheduler_type': 'constant', 'fp16': True, 'load_best_model_at_end': True, 'eval_strategy': 'epoch', 'metric_for_best_model': 'eval_loss', 'save_strategy': 'epoch', 'save_total_limit': 1, 'save_safetensors': True, 'output_dir': '/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}', 'wandb_project': 'test', 'wandb_runname': 'test', 'lamb': 0.333, 'margin': 3.093, 'lamb_intra': 3.339}, 'model': {'id': 0, 'model_type': 'hybrid', 'model_name_or_path': 'bert-base-uncased'}}\n",
            "[2025-03-22 08:47:02,928][__main__][INFO] - Load dataset.\n",
            "[2025-03-22 08:47:04,907][__main__][INFO] - Done with loading the dataset.\n",
            "Some weights of HybridBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2025-03-22 08:47:05,694][utils.utils_models][INFO] - loaded HybridBERT constraction\n",
            "Map: 100% 1071/1071 [00:01<00:00, 807.53 examples/s]\n",
            "Map: 100% 356/356 [00:00<00:00, 780.91 examples/s]\n",
            "Map: 100% 356/356 [00:00<00:00, 797.34 examples/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2188: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'regression_scaled_loss': np.float64(0.014073573052883148), 'regression_loss': np.float64(0.028147146105766296), 'epoch': 1.0}\n",
            "{'classification_scaled_loss': np.float64(0.9319450855255127), 'classification_loss': np.float64(1.8638901710510254), 'epoch': 1.0}\n",
            "100% 134/134 [01:44<00:00,  1.31it/s]\n",
            "  0% 0/45 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/45 [00:00<00:04,  8.66it/s]\u001b[A\n",
            "  7% 3/45 [00:00<00:07,  5.99it/s]\u001b[A\n",
            "  9% 4/45 [00:00<00:08,  5.11it/s]\u001b[A\n",
            " 11% 5/45 [00:00<00:08,  4.60it/s]\u001b[A\n",
            " 13% 6/45 [00:01<00:08,  4.47it/s]\u001b[A\n",
            " 16% 7/45 [00:01<00:08,  4.39it/s]\u001b[A\n",
            " 18% 8/45 [00:01<00:08,  4.31it/s]\u001b[A\n",
            " 20% 9/45 [00:01<00:08,  4.21it/s]\u001b[A\n",
            " 22% 10/45 [00:02<00:08,  4.18it/s]\u001b[A\n",
            " 24% 11/45 [00:02<00:08,  4.18it/s]\u001b[A\n",
            " 27% 12/45 [00:02<00:07,  4.17it/s]\u001b[A\n",
            " 29% 13/45 [00:02<00:07,  4.14it/s]\u001b[A\n",
            " 31% 14/45 [00:03<00:07,  4.13it/s]\u001b[A\n",
            " 33% 15/45 [00:03<00:07,  4.14it/s]\u001b[A\n",
            " 36% 16/45 [00:03<00:07,  4.13it/s]\u001b[A\n",
            " 38% 17/45 [00:03<00:06,  4.12it/s]\u001b[A\n",
            " 40% 18/45 [00:04<00:06,  4.12it/s]\u001b[A\n",
            " 42% 19/45 [00:04<00:06,  4.20it/s]\u001b[A\n",
            " 44% 20/45 [00:04<00:05,  4.17it/s]\u001b[A\n",
            " 47% 21/45 [00:04<00:05,  4.17it/s]\u001b[A\n",
            " 49% 22/45 [00:05<00:05,  4.15it/s]\u001b[A\n",
            " 51% 23/45 [00:05<00:05,  4.16it/s]\u001b[A\n",
            " 53% 24/45 [00:05<00:05,  4.15it/s]\u001b[A\n",
            " 56% 25/45 [00:05<00:04,  4.14it/s]\u001b[A\n",
            " 58% 26/45 [00:06<00:04,  4.13it/s]\u001b[A\n",
            " 60% 27/45 [00:06<00:04,  4.13it/s]\u001b[A\n",
            " 62% 28/45 [00:06<00:04,  4.13it/s]\u001b[A\n",
            " 64% 29/45 [00:06<00:03,  4.13it/s]\u001b[A\n",
            " 67% 30/45 [00:07<00:03,  4.11it/s]\u001b[A\n",
            " 69% 31/45 [00:07<00:03,  4.11it/s]\u001b[A\n",
            " 71% 32/45 [00:07<00:03,  4.12it/s]\u001b[A\n",
            " 73% 33/45 [00:07<00:02,  4.12it/s]\u001b[A\n",
            " 76% 34/45 [00:07<00:02,  4.12it/s]\u001b[A\n",
            " 78% 35/45 [00:08<00:02,  4.11it/s]\u001b[A\n",
            " 80% 36/45 [00:08<00:02,  4.11it/s]\u001b[A\n",
            " 82% 37/45 [00:08<00:01,  4.13it/s]\u001b[A\n",
            " 84% 38/45 [00:08<00:01,  4.12it/s]\u001b[A\n",
            " 87% 39/45 [00:09<00:01,  4.10it/s]\u001b[A\n",
            " 89% 40/45 [00:09<00:01,  4.11it/s]\u001b[A\n",
            " 91% 41/45 [00:09<00:00,  4.12it/s]\u001b[A\n",
            " 93% 42/45 [00:09<00:00,  4.10it/s]\u001b[A\n",
            " 96% 43/45 [00:10<00:00,  4.10it/s]\u001b[A\n",
            " 98% 44/45 [00:10<00:00,  4.11it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7401813045895507, 'eval_runtime': 10.8713, 'eval_samples_per_second': 32.747, 'eval_steps_per_second': 4.139, 'epoch': 1.0}\n",
            "100% 134/134 [01:55<00:00,  1.31it/s]\n",
            "100% 45/45 [00:10<00:00,  4.81it/s]\u001b[A\n",
            "{'train_runtime': 124.0094, 'train_samples_per_second': 8.636, 'train_steps_per_second': 1.081, 'train_loss': 0.9568988173755247, 'epoch': 1.0}\n",
            "100% 134/134 [02:04<00:00,  1.08it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/conf/wandb/offline-run-20250322_083817-0rsm6sp1\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250322_083817-0rsm6sp1/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_2\n",
        "!python train.py -m data=asap data.prompt_id=2 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.0069 +training.margin=0.752 +training.lamb_intra=0.069 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "01VG-vNoAu9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_3\n",
        "!python train.py -m data=asap data.prompt_id=3 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.006 +training.margin=0.956 +training.lamb_intra=0.06 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "zutWUK1RBTT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_4\n",
        "!python train.py -m data=asap data.prompt_id=4 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.0099 +training.margin=0.192 +training.lamb_intra=0.099 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "8lPn2LOJBhW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_5\n",
        "!python train.py -m data=asap data.prompt_id=5 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.0475 +training.margin=3.559 +training.lamb_intra=0.475 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "LHQ4kNylBte-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_6\n",
        "!python train.py -m data=asap data.prompt_id=6 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.0483 +training.margin=5.117 +training.lamb_intra=0.483 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "qH_UUsnmB6PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_7\n",
        "!python train.py -m data=asap data.prompt_id=7 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.9121 +training.margin=8.914 +training.lamb_intra=9.121 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "SMO4XAb5COMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prompt_8\n",
        "!python train.py -m data=asap data.prompt_id=8 data.fold=0,1,2,3,4 model.model_type='hybrid' model.id=0 model.model_name_or_path='bert-base-uncased' +training.lamb=0.7881 +training.margin=6.582 +training.lamb_intra=7.881 training.num_train_epochs=1 training.output_dir='/content/hybrid_bert/prompt_${data.prompt_id}/fold${data.fold}' training.wandb_project='test' training.wandb_runname=\"test\""
      ],
      "metadata": {
        "id": "MHImiXwrCYlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}