training: 
    weight_decay: 0.0
    learning_rate: 1e-5
    per_device_eval_batch_size: 8
    per_device_train_batch_size: 8
    num_train_epochs: 10
    lr_scheduler_type: constant
    lamb: ???
    margin: ??? 
    lamb_intra: ???

data: ???
model: ???